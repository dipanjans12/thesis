\chapter{Conclusions and Future Directions}

In this thesis we have presented and evaluated system design principles for efficient scheduling and resource management in heterogeneous, accelerator-based systems with 1000s of compute cores and complicated memory hierarchy in order to achieve better core utilization, efficient data movement and seamlessly scaling to large input datasets, particularly those arising from the processing complex GPU-based applications. The proposed technologies address this resource management and scheduling crisis by bringing support for load balanced scheduling of application requests to avoid request collisions, feedback-based mechanisms for efficient data movement and placement, system-level support for reducing core idling and seamlessly scaling to large input datasets for out-of-core processing to achieve optimal performance in a wide variety of applications.

We presented Strings, a cluster-wide GPU aggregation and two-level hierarchical scheduling infrastructure that decomposes the problem of GPGPU request scheduling into a novel combination of workload balancing and device-level scheduling. To achieve high GPU utilization and minimize context switching overhead, it provides system-level support to dynamically encapsulate the GPU contexts of multiple applications into a single umbrella context.  Further dynamic policy switching based on device-level scheduler feedback and advanced scheduling  policies like Phase Selection (PS) that aims to keep all of a GPUâ€™s hardware units busy by intelligently selecting applications operating in different phases of their use of the GPU, achieve high system throughput without compromising with the fairness among multi-tenant applications. Across a wide variety of workloads and system configurations,  Strings achieves substantial throughput and fairness improvement compared to the CUDA runtime and competing GPU scheduling solutions. 

This dissertation addresses the technical challenges in large-scale graph analytics including dealing with the dynamic nature of graph parallelism, coping with constrained on-GPU memory capacity and addressing programmability issues for developers with limited insights into how to best exploit the resources of evolving and varied GPU architectures. Towards this end, we have developed GraphReduce framework that enables processing of graphs with memory footprint much exceeding that of GPU memory, by sharding graph data and asynchronously moving shards between GPU and host memories. Technical advances offered by GraphReduce include its usage of a hybrid programming model of edge- and vertex-centric processing, asynchronous execution/spray operation, dynamic phase fusion/elimination, and dynamic frontier management. With these optimizations, GraphReduce achieves significant performance improvement over competing out-of-core implementations of graph processing  for several real-world large-scale graphs processed by various algorithms. 

To address the problem of anlayzing dynamic graphs that are changing over time we have presented EvoGraph, an accelerator-based high-performance incremental graph processing framework for processing time-evolving graphs. Using its novel programming model I-GAS that is based on Gather- Apply-Scatter model, EvoGraph computes graph properties only for the inconsistent subgraphs. Further with a user-tunable property-based optimization called property-guard for switching between incremental and static recomputation and deep memory copy operations via separate CUDA Streams, EvoGraph achieves improved system throughput when compared to competing streaming graph processing frameworks. 

\textit{In summary, this dissertation provides hard evidence for the thesis statement, specifically that new system design principles are required for efficient scheduling and fine grain resource management in heterogeneous many-core system, to harness the full potential of future exascale machines with compute nodes expected to be comprised of 1000s of accelerator and general purpose cores for the increasingly performance hungry applications with varied memory access pattern.}

As an ongoing effort, we are exploring several extensions to our work in this dissertation. On the GPU sharing front, we are considering dynamic opportunities and tradeoffs in mapping executions to either GPUs or CPUs, using runtime methods for dynamic binary translation. Another interesting extension would be further exploration of the effects of data movement on program performance and consequent changes in scheduling policies for discrete vs. integrated GPUs. From the graph analytics viewpoint, we are investigating to extend our solutions to support multiple on-node GPUs, going beyond single node processing to multi-node clusters and extreme-scale datasets. Specifically, understanding different kinds of consistency guarantees that are required to enable processing of evolving graph in a distributed setup.


