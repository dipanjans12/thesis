\section{Thesis Statement}
\label{thesis}
The future exascale machines with compute nodes are expected to be comprised of millions of heterogeneous accelerator-based and general purpose cores. It is a huge challenge to efficiently schedule applications and place the appropriate data near the computation. To address this resource management and scheduling crisis we must build system-level design and abstractions that support load balanced scheduling of application requests to avoid request collisions, feedback-based mechanisms for efficient data movement and placement, system-level support for reducing core idling and seamlessly scaling to large input datasets for out-of-core processing to achieve optimal performance in a wide variety of applications.   

\section{Contributions}
The key contribution of our research is a set of technologies that addresses the aforementioned challenges. Specifically, to validate the thesis, we make the following contributions:
\begin{itemize}
\item \textbf{Strings Scheduler.} To address the challenges in scheduling multi-tenant cloud workloads in the heterogeneous resources of future, high-end manycore GPU-based server platforms we design and implement the Strings scheduler, a two-level hierarchical scheduler that decomposes the scheduling problem into a combination of load balancing and per-device resource sharing. The workload balancing intelligently binds each application’s GPU component to an appropriate GPU and the device-level, per-GPU scheduler handles GPU resource sharing for the multiple tenants mapped to a single GPU, to improve application performance while also meeting system-level goals like high throughput, fairness, etc. It implements a model in which accelerators like GPUs are first class schedulable entities rather than statically chosen devices used as single SIMD engines. The intent is to avoid the serial execution of GPU contexts that could otherwise have been executed concurrently, as with the varying workloads imposed by web applications, where during peak demands, the current model of static GPU assignments will cause some GPU devices to be heavily utilized while others are idle or underutilized. Explicit scheduling can also avoid underutilization caused by the differences in the fraction of CPU vs. GPU components seen across different applications, for reasons that include an inability to parallelize certain application components and/or limited GPU residency vs. the costs of host-GPU data transfers. Strings makes GPUs into explicitly scheduled entities by overriding the device selection calls made by applications. It then manages these calls with the aforementioned two-level scheduler, at the top, balancing workloads across the multiple GPUs resident in each manycore node, and at the device level, reducing GPU core idling via GPU multi-tenancy and the judicious overlap of GPU execution with host-GPU data movements. Strings also supports true GPU multi-tenancy, termed the `Context Packing', which dynamically packs the GPU contexts of multiple applications into a single context, to achieve high GPU utilization and low context switching overhead. Additional methods enable in providing dynamic feedback from device-level schedulers to workload balancer, to inform the global decisions made by the latter about characteristics of the applications being scheduled by the former.

%Strings makes it easy to experiment with and evaluate alternative scheduling strategies for future heterogeneous manycore platforms. In this thesis, using workloads drawn from diverse classes of GPGPU applications, experimental results obtained with Strings are obtained for three different workload balancing strategies, three GPU scheduling policies, and four feedback policies. GPU scheduling policies presented and evaluated using Strings are distinct from prior work in their explicit consideration of data movement to/from the GPU device. The Phase Selection (PS) policy, which maximizes GPU utilization by smartly selecting and co-scheduling applications that currently operate in different phases computation vs. communication - of their combined CPU/GPU execution. Advanced feedback-based policies, termed Data Transfer Feedback (DTF) and Memory Bandwidth Feedback (MBF), that exploits the advantages offered by CUDA streams by collocating applications with contrasting behavior, in terms of data transfer and memory intensity, to achieve extreme performance benefits.

\item \textbf{GraphReduce Framework.} To address the problem of processing graph applications with larger memory footprint than the device memory, we present GraphReduce (GR), a highly efficient and scalable GPU-based  out-of-core graph processing framework that operates on graphs that exceed the device’s internal memory capacity. GraphReduce supports an access pattern based hybrid computational model adopting a combination of edge- and vertex-centric implementations of the Gather-Apply-Scatter (GAS) programming model to match the different types of parallelism present in different phases of the GAS execution model. GR achieves efficiency in graph processing via improved asynchrony in computation and communication (operating on multiple asynchronous GPU streams), by dynamic characterization of data buffers based on data access pattern and access locality to fully exploit the high degrees of parallelism in GPUs.   Additional hardware parallelism is extracted via \textit{spray streams} for deep copy operations on separate CUDA streams. GR runtime also uses computational frontier information for efficient GPU hardware thread scheduling and data movement between host and GPU. Specifically, GR moves data into GPU memory only when a subset of the graph has at least one active vertex or edge. Further, when possible, GR uses dynamic phase fusion/elimination to merge/eliminate multiple GAS phases, to avoid unnecessary kernel launches and associated data movement.

\item \textbf{EvoGraph.} Because of the extreme scale of real-world graphs and the high rate at which they evolve combined with the complexity of user queries, traditional processing methods by first storing the updates and then repeatedly running static graph analytics on a sequence of snapshots are deemed undesirable and computational infeasible on GPUs. To address such challenges of processing real-world graphs that are constantly evolving over time we present the design and implementation of EvoGraph, a high performance dynamic graph analytics framework for evolving graph analytics on GPUs that incrementally processes graphs on-the-fly using fixed-sized batches of updates.    As part of GraphIn, we propose a novel programming model called I-GAS that is based on the gather-apply-scatter programming paradigm and that allows for implementing a large set of incremental graph processing algorithms seamlessly across multiple GPU cores.  We further propose novel optimizations like property-based dual path execution in the EvoGraph framework to choose between an incremental vs static run over a particular update batch and GPU 'context merging' to merge and collocate the GPU contexts of static and incremental graph algorithms on the same GPU, inorder to avoid context switching overhead and efficiently use of all hardware resources using GPU streams, including its computational and data movement engines.

\item An extensive performance evaluation of each of the above runtime frameworks on wide variety of workloads and algorithms to demonstrate their effectiveness when compared to state-of-the-art competing solutions. 

\end{itemize}

\section{Dissertation Structure}
The rest of this dissertation is organized as follows. 

Chapter 2 discusses the design and implementation of Strings, a hierarchical scheduling framework for efficient sharing
and scheduling of multi-tenant cloud workloads on multi-GPU server systems. 

%Chapter 3 discusses the salient research related to the systems and topics dealing with graph processing, including accelerator-based and real-time streaming graph processing. The chapter also motivates the use of GPUs in high performance graph analytics and their impact on the software design.

Chapter 3 presents the design and implementation of Graphreduce, a framework for large-scale out-of-core graph processing using GPUs where
the input graph may or may not fit in GPU memory, supporting access pattern based
hybrid computational model and efficient data movement techniques.

Chapter 4 explains in detail the design and implementation of EvoGraph, a high performance dynamic graph processing framework for evolving
graph analytics using GPUs that incrementally processes graphs on-the-fly using fixed-sized batches of updates. 

Each of the above chapters also include a detailed performance evaluation of each of the  runtime frameworks presented above on wide variety of workloads and algorithms validating their effectiveness when compared to state-of-the-art competing solutions.

Chapter 5 discusses the salient research related to the systems and topics dealing with graph processing, including accelerator-based and real-time streaming graph processing. 

Chapter 6  concludes the dissertation and presents future avenues of research.