Accelerator-based systems are making rapid inroads into becoming platforms of choice for both high-end cloud services and processing applications with irregular access pattern such as real-world graph analytics, due to their high scalability and low dollar to FLOPS ratios. Yet GPUs are not first class schedulable entities causing substantial hardware resource underutilization, including their computational and data movement engines. Therefore, software solutions with support for efficient resource management principles are required to address such scheduling crisis in GPUs. Further, two important characteristics of real world graphs like those in social networks are that they are big and are constantly evolving over time. This poses challenge due to limitations in GPU-resident memory for storing these large graphs. And because of the high rate at which these large-scale graphs evolve, it is undesirable and computationally infeasible to repeatedly run static graph analytics on a sequence of versions, or snapshots, of the evolving graph. 
Therefore, novel incremental solutions are required to process large-scale evolving graphs in near real-time using GPUs with memory footprint exceeding the device's internal memory capacity. 

First, to address the challenges of GPU multi-tenancy, the thesis presents \textit{Strings} scheduler for heterogeneous manycore nodes that implements a model in which GPUs are treated as first class schedulable entities, by decomposing the scheduling problem into a combination of load balancing and per-device resource sharing. Its utility as an infrastructure for developing and evaluating advanced scheduling methods is demonstrated for server workloads, where (i) load balancing intelligently binds each application’s GPU component to an appropriate GPU and, (ii) device-level sharing aims to keep all of a GPU’s hardware units busy, by concurrently running those applications that reside in different phases of their use of the GPU. It also prioritizes GPU requests that have attained `least service' to achieve high system throughput, and goes beyond that to also ensure fairness via a history based fair-share scheduler. Over a wide variety of multi-tenant workloads, Strings achieves substantial speedups compared to that obtained by the native CUDA runtime and other competitive GPU schedulers. 

Second, to address the problem of processing graph applications with larger memory footprint than the device memory the thesis presents \textit{GraphReduce}, a highly efficient and scalable GPU-based framework that adopts a combination of edge- and vertex-centric implementations of the Gather-Apply-Scatter programming model and operates on multiple asynchronous GPU streams to fully exploit the high degrees of parallelism in GPUs supporting efficient graph data movement between the host and device. GraphReduce (GR) runs graph algorithms on GPUs without unduly burdening graph algorithm developers. Programmers write the appropriate sequential codes for their graph algorithms and then use its simple APIs to express their use for processing various graphs. The GR runtime seamlessly partitions the graph into different shards, each single one of which entirely fits into GPU memory, and overlaps shard movement with GPU-level graph processing, the latter using multiple levels of GPU-level parallelism. With such automation, GR can deal with graph sizes much exceeding GPU memory sizes. Extensive experimental evaluations for a wide variety of graph inputs and algorithms demonstrate that GraphReduce significantly outperforms other competing out-of-memory approaches.


Finally, we address the problem of processing real-world graphs that are constantly evolving over time using GPUs. Although modern GPUs provide massive amount of parallelism for efficient graph processing, the challenges remain due to their lack of support for this near real-time streaming nature of dynamic graphs. Specifically, because of the current high volume and velocity of graph data combined with the complexity of user queries, traditional processing methods by first storing the updates and then repeatedly running static graph analytics on a sequence of versions or snapshots are deemed undesirable and computational infeasible on GPU. To address this problem of analyzing evolving graphs in near real- time, we present \textit{EvoGraph}, a highly efficient and scalable GPU-based dynamic graph analytics framework that incrementally processes graphs on-the-fly using fixed-sized batches of updates. To realize this vision, we propose a novel programming model called I-GAS that is based on the gather-apply-scatter programming paradigm and that allows for implementing a large set of incremental graph processing algorithms seamlessly across multiple GPU cores. Further we propose novel optimizations like property-based dual path execution in the EvoGraph framework to choose between an incremental vs static run over a particular update batch and GPU 'context merging' to efficiently use of all hardware resources using GPU streams, including its computational and data movement engines. Extensive experimental evaluations for a wide variety of graph inputs and algorithms demonstrate that EvoGraph achieves substantial speedup compared to static graph recomputation and other competing streaming graph processing frameworks such as STINGER.
