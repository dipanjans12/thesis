\chapter{Introduction}
\label{intro}

High performance machines are increasingly using GPUs~\cite{15, 16, 17, 18, 19}, to leverage their scalability and low dollar to FLOPS ratios.  As a result, GPUs have become the main compute engines for today’s HPC clusters and supercomputers like the Titan supercomputer in Oak Ridge~\cite{titan}. This trend continues with the move toward exascale machines~\cite{exascale1, exascale2, exascale3, exascale4, exascale5, exascale6, exascale7}, with compute nodes expected to be comprised of millions of accelerator and general purpose cores, whether packaged as `thin' or `fat' nodes (shown in Figure~\ref{fig:hybrid}).  Therefore, it is not only important to efficiently schedule applications to keep all the available cores busy but also  intelligently move the appropriate data near the computation as these accelerators have limited amount of memory attached to them. Existing software infrastructures deal very poorly in terms of scheduling and fine grain resource management of such heterogeneous architecture leading to substantial underutilization of all the available resources, including both the computational and data movement engines. Next we describe three broad classes of applications where there is substantial GPU underutilization and showcase the current scheduling crisis in them. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.70\textwidth]{hybrid}
\caption{Accelerator-based heterogeneous system architecture }
\label{fig:hybrid}
\end{center}	
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.70\textwidth]{servicemodel}
\caption{GPGPU application service model following a negative exponential
distribution of request arrival from multiple end users. }
\label{fig:service}
\end{center}	
\end{figure}

\section{GPU Sharing in Cloud}
A recent trend is the gain in popularity of computationally intensive high performance applications in client-server workloads including image processing algorithms like video transcoding~\cite{element} , financial algorithms~\cite{zillians}, online gaming, e.g., NVIDIA’s cloud gaming~\cite{nvidia-game}, search~\cite{GPUsearch}, data mining~\cite{GPUmine} and multimedia services like Adobe’s Photoshop.com~\cite{adobe}. This motivates online services to take advantages of GPU clusters. This is mirrored by GPU offerings by cloud providers like Amazon ECC~\cite{amazon}, Nimbix~\cite{nimbix}, Peer1 Hosting~\cite{peer1}, and Penguin Computing~\cite{penguin}.  Figure~\ref{fig:service} shows the application service model for a multi-tenant single GPU server.

A challenge to using GPUs in these multi-tenant server and cloud environment is the lack of sophisticated support for GPU scheduling, given the predominant model of treating GPUs as statically chosen devices, in which applications explicitly and programmatically select the GPU devices on which they wish to run, rather than as first class schedulable entities~\cite{pegasus}. Such static GPU assignments will inhibit concurrency, particularly with the varying workloads imposed by web applications. For instance, during peak demands for certain services, some GPU devices will be heavily utilized while other services’ GPUs will be idle or underutilized. Low GPU utilization can also be attributed to considerable diversity in the fraction of CPU vs. GPU component in applications, for reasons that include an inability to parallelize certain application components and/or limited GPU residency vs. the costs of host-GPU data transfers. Finally, although each GPU can internally contain thousands of cores, it is treated by applications as a single SIMD engine, potentially resulting in the serial execution of GPU contexts that could have been executed concurrently.

\section{Large-Scale Graph Analytics on GPUs}
With the increasing interest in many emerging domains such as social networks, the World Wide Web (e-commerce and advertising), and genomics, the importance of graph processing has grown substantially. Some examples of graph analytics include friend/product recommendations, anomaly and trend detection, online advertisement serving etc. This recent trend has given rise to many graph processing frameworks in both distributed, e.g. GraphLab~\cite{graphlab}, PowerGraph~\cite{powergraph},  Pregel~\cite{pregel}; and single machine shared-memory environments, e.g.  Graphchi~\cite{chi}, X-Stream~\cite{xstream}, Ligra~\cite{ligra} etc. This need to rapidly process large graph-structured data has also engendered recent efforts to leverage cost-efficient GPUs for efficient graph analytics. Doing so, however, requires addressing substantial technical challenges, including (1) dealing with the dynamic nature of graph parallelism, (2) coping with constrained on-GPU memory capacity, i.e., to process graphs with memory footprints that exceed that capacity, and (3) addressing programmability issues for developers with limited insights into how to best exploit the resources of evolving and varied GPU architectures.

More precisely, a graph processing framework using GPUs should expose abstractions or simple APIs for the developers to write the appropriate sequential codes for their domain specific algorithms, e.g., for data mining, machine learning, etc to express their use for processing graphs of arbitrary size. The runtime should then seamlessly (i) partition the graphs into smaller chunks each single one of which entirely fits into GPU memory, (ii) efficiently move data between host and device leveraging concurrent GPU operations to obtain fine-grain parallelism that exploits both GPU software and hardware features like CUDA streams and Hyper-Q of Kepler GPUs etc (iii) choosing the most appropriate programming model ( edge- or vertex- centric or a combination of both) to generate device code for efficient GPU-level graph processing, and iv) finally intelligent coordinated scheduling and management of both the data movement and compute engines to achieve optimal performance. With such automation, we can deal with graph sizes much exceeding GPU memory sizes. This is important because even a common Yahoo web-graph comprised of 1.4 billion vertices requires approximately 6.6 GB of memory to store just its vertex values (not even including the edges and their corresponding states).

In summary, the goal is to  design a scale-up graph processing framework on HPC systems with discrete GPUs and high end (i.e., memory-rich) hosts where GPUs can be used to accelerate analytics performed on graphs with billions of edges, operating at speeds much exceeding that of similar operations run on CPUs, and programmed in ways accessible to programmers who are not experts in GPU programming. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.70\textwidth]{incremental-graph}
\caption{Static vs. incremental graph processing. }
\label{fig:inc}
\end{center}	
\end{figure}

\section{Dynamic Graph Analytics on GPUs}
Another important aspect of real-world graphs like facebook friend lists or twitter follower graphs is that they dynamically change with time.  Current graph analytics on such dynamic graphs follow a store-and-static-compute model that involves first storing batches of updates to a graph applied at different points in time and then repeatedly running static graph computations on multiple versions or snapshots of this evolving graph sequence. The key assumption made here is that the rate of change in graphs due to continuous updates is slower than the execution time of the static graph analytics. This assumption might not hold true for current real-world graphs. For instance, Twitter traffic can peak at 143 thousand tweets (and associated updates) / per second and emails sent per second can reach as high as 2.5 millions/sec.   Hence, there are two fundamental challenges to applying static recomputation to these types of rapidly changing data sets. First, static graph analytics on a single version of the evolving graph, even when leveraging massive amount of parallelism offered by multiple cores in a high performance cluster, can be very slow due to the extreme scale of many real-world graphs and/or because of the complexity of the graph queries that are traditionally both compute and memory intensive. Therefore, the cumulative cost of analyzing such large-scale versions with complex graph queries repeatedly can be substantially high. Second, there are real world graph analytics problems that inherently require soft or hard real time guarantees, e.g., real-time anomaly detection, disease spreading etc. So to conclude, the current high volume and velocity of graph data combined with the complexity of user queries has outstripped the traditional static graph analytics model on streaming graphs.

To address the above mentioned computational crisis in dynamic graph processing we need a graph processing framework that can incrementally process a continuous stream of updates (i.e., edge/vertex insertions and deletions) as a sequence of batches.  Because the incremental logic, in many practical scenarios, affects only a portion of the graph (as shown in Figure~\ref{fig:inc}), this reduction can result in tremendous performance benefit compared to static recomputation of the graph algorithm on the entire graph for many popular graph algorithms and real-world graphs.
Further, we also need to handle the scenarios when updates to the graph affects a very large portion of the graph and incremental processing won’t help much or may even be worse (due to overheads of incremental execution)  compared to a static recomputation. E.g., in incremental BFS, updates that affect vertices close to the root node affect nearly the entire BFS tree. In this case, the incremental run can at best perform as good as the static re-run.  Hence a characterization of graph algorithm that would benefit the most from incremental processing is essential. Finally, in order to allow faster updates to the graph and run both the incremental and static graph algorithms efficiently on GPUs, we need to design appropriate data structures specifically tailored to store both the graph and the updates for  efficient scheduling and data movement between the host and the device. 

\input{thesis} 